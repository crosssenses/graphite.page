# Executive Summary 

A substantial portion of contemporary public discourse and social interaction is conducted over online social media platforms, such as Facebook, YouTube, Reddit, and TikTok. Accordingly, these platforms form a core component of the digital public sphere[: REFERENCE | Schaefer2015 | :] which, although subject to private ownership, constitute a digital infrastructural resource[: REFERENCE | Grimmelmann | :] that is open to members of the public. As private entities, platforms can set their own rules for participation, in the form of terms of service, community standards, and other guidelines. The content moderation systems deployed by such platforms to ensure that content posted on the platform complies with these terms, conditions, and standards have the potential to influence and shape public discourse by mediating what members of the public are able to see, hear, and say online. Over time, these rules may have a norm-setting effect, shaping the conduct and expectations of users about what acceptable discourse looks like. Thus, the design and implementation of content moderation systems have a powerful impact on the freedom of expression of users and their access[: SIDENOTE | :] to dialogic interaction on the platform. With great power comes great responsibility: the increasing trend towards the adoption of algorithmic content moderation systems that have a questionable track record as regards their ability to safeguard freedom of expression gives rise to urgent concerns on the need to ensure that content moderation is regulated in a manner that safeguards and fosters robust public discourse in the online sphere. 
:--- NOTE ---:
Content moderation can exclude individuals participating in discourse taking place over the platform, e.g. by blocking user accounts or blacklisting individuals from participating in the discourse taking place on the platform.
:------------:

This policy brief was developed with the contributions of an interdisciplinary team of experts from the fields of law, political science, sociology, and engineering. It aims to inform legislators and policymakers of the risks posed by the proliferation of algorithmic content moderation and the need for a more proactive regulatory approach by the states towards the governance of content moderation systems that are deployed by online platforms providing digital infrastructure for public discourse. It highlights an *information gap* that prevents regulators from evaluating the impact of content moderation on freedom of expression and an *accountability gap* that arises through the absence of effective redress mechanisms by which users are able to challenge violations of freedom of expression. 

The policy brief concludes by proposing strategies for bridging these information and accountability gaps by means of:

**Introducing enforceable statutory obligations** which require platforms to: 

* Provide information on the design, implementation, and impact of their content moderation systems through periodic reports and fundamental rights impact assessments (designed to address the information gap).

* Increase the capacity for individual redress through effective complaints and redress mechanisms (designed to address the accountability gap).

**Establishing an Ombudsperson** who is authorized to supervise the protection of freedom of expression on online platforms. 

**Facilitating informed, society-wide debate** about how content moderation systems should be designed and optimised. 