<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/Article">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Place this data between the <head> tags of your website -->
<title>Disclosure Rules for Algorithmic Content Moderation</title>
<meta name="description" content="A call for a multi-level transparency regime for social media platforms" />
<meta name="author" content="A policy paper by fellows of the HIIG research sprint on AI and content moderation">

<!-- Schema.org markup for Google+ -->
<meta itemprop="name" content="Disclosure Rules for Algorithmic Content Moderation">
<meta itemprop="description" content="A call for a multi-level transparency regime for social media platforms">
<meta itemprop="image" content="https://graphite.page/policy-brief-blackbox/assets/images/social.png">

<!-- Twitter Card data -->
<meta name="twitter:title" content="Disclosure Rules for Algorithmic Content Moderation">
<meta name="twitter:description" content="A call for a multi-level transparency regime for social media platforms">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@author_handle">
<!-- Twitter Summary card images must be at least 120x120px -->
<meta name="twitter:image" content="https://graphite.page/policy-brief-blackbox/assets/images/social.png">

<!-- Open Graph data -->
<meta property="og:title" content="Disclosure Rules for Algorithmic Content Moderation" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://graphite.page/policy-brief-blackbox/" />
<meta property="og:image" content="https://graphite.page/policy-brief-blackbox/assets/images/social.png" />
<meta property="og:description" content="A call for a multi-level transparency regime for social media platforms" /> 
<meta property="og:site_name" content="graphite – Enhanced publications for a digital environment" />

<!--
<meta name="twitter:site" content="@publisher_handle">
<meta property="fb:admins" content="Facebook numeric ID" />
-->
    
      <link rel="stylesheet" href="theme/styles/sample-journal.css">
    
  </head>
  <body>

    
<!-- Navigation to other graphite content sliding in from top -->
<div id="navContent" style="display: none;">
  <div class="container">
    <div class="row">

      <div class="col-md-6">
        <p class="ms-nav-info">The NoC (Global Network of Internet and Society Research Centers) research project <em>The Ethics of Digitalisation - From Principles to Practices</em> promotes an active exchange and aims to foster a global dialogue on the ethics of digitalisation by involving stakeholders from academia, civil society, policy, and the industry. The three policy briefings form the outputs of the research sprint on AI and content moderation which was hosted virtually by the <a href="https://www.hiig.de/en">HIIG</a> from August to October 2020.</p>
      </div>

      <div class="col-md-3">
        <h3 class="ms-nav-section">All policy papers</h3>
        <ul class="ms-nav-subnav">
          <li>
            <a href="https://graphite.page/policy-brief-audits">Making Audits Meaningful</a>
          </li>
          <li>
            <a href="https://graphite.page/policy-brief-blackbox">Disclosure Rules for Algorithmic Content Moderation</a>
          </li>
          <li>
            <a href="https://graphite.page/policy-brief-values">Freedom of Expression in the Digital Public Sphere</a>
          </li>
        </ul>
      </div>

      <div class="col-md-3">
        <h3 class="ms-nav-section">More information</h3>
        <ul class="ms-nav-subnav">
          <li>
            <a href="https://www.hiig.de/en/project/the-ethics-of-digitalisation" target="_blank">Project website of Ethics of Digitalisation</a>
          </li>
          <li>
            <a href="https://www.hiig.de/en/dossier/digitalisation-for-the-common-good" target="_blank">More on the topic</a>
          </li>
        </ul>
      </div>

    </div>
  </div>
</div>

<!-- Navigation bar fixed to top -->
<nav class="ms-navigation-top">
  <div class="container">
    <span class="ms-article-title">
      Disclosure Rules for Algorithmic Content Moderation
    </span>

    <a class="ms-brand" href="https://www.impactdistillery.com/graphite">
      runs on <object data="theme/images/graphite.svg" type="image/svg+xml"></object>
    </a>

    <span id="navToggle" class="navbar-toggler-icon"></span>

  </div>
</nav>

    
<!-- START: STUDY FRAME -->
<div class="ms-header"
     style="background-image: url(assets/images/ricardo-gomez-angel-xeo82lLsXtY-unsplash.jpg);">
  <div class="container">
    <div class="card">
      <div class="card-body">
        <p class="author">Aline Iramina, Charlotte Spencer-Smith, Wai Yan</p>
        <h1 class="card-title">Disclosure Rules for Algorithmic Content Moderation</h1>
        <p class="card-subtitle">A call for a multi-level transparency regime for social media platforms</p>
      </div>
    </div>
  </div>
</div>
<footer class="ms-footer ms-footer-sticky"><div class="container">A policy paper by fellows of the HIIG research sprint on AI and content moderation</div></footer>

<div class="ms-toc"></div>
<nav class="ms-tabs">
  <div class="container">

    <a class="ms-trigger-toc button d-flex d-lg-none">
      Content
    </a>

    <div class="dropdown d-xs-block d-lg-none">
      <button class="btn btn-secondary dropdown-toggle" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
        Article
      </button>

      <div class="dropdown-menu" aria-labelledby="dropdownMenuButton" role="tablist">
        <div class="nav nav-pills" role="tablist">
          
          <a class="dropdown-item tab-item active" id="index-tab" data-toggle="tab" href="#index" role="tab" aria-controls="index" aria-expanded="true">
            Article
          </a>
          
          <a class="dropdown-item tab-item " id="authors-tab" data-toggle="tab" href="#authors" role="tab" aria-controls="authors" >
            Authors
          </a>
          
          <a class="dropdown-item tab-item " id="directories-tab" data-toggle="tab" href="#directories" role="tab" aria-controls="directories" >
            References
          </a>
          
          <a class="dropdown-item tab-item " id="appendices-tab" data-toggle="tab" href="#appendices" role="tab" aria-controls="appendices" >
            Appendices
          </a>
          
          <a class="dropdown-item tab-item " id="editors-tab" data-toggle="tab" href="#editors" role="tab" aria-controls="editors" >
            About
          </a>
          
        </div>
      </div>
    </div>

    <ul class="nav nav-tabs d-none d-lg-flex" id="masterTab" role="tablist">
      
      <li class="nav-item">
        <a class="nav-link tab-item active" id="index-tab" data-toggle="tab" href="#index" role="tab" aria-controls="index" aria-expanded="true">
          Article
        </a>
      </li>
      
      <li class="nav-item">
        <a class="nav-link tab-item " id="authors-tab" data-toggle="tab" href="#authors" role="tab" aria-controls="authors" >
          Authors
        </a>
      </li>
      
      <li class="nav-item">
        <a class="nav-link tab-item " id="directories-tab" data-toggle="tab" href="#directories" role="tab" aria-controls="directories" >
          References
        </a>
      </li>
      
      <li class="nav-item">
        <a class="nav-link tab-item " id="appendices-tab" data-toggle="tab" href="#appendices" role="tab" aria-controls="appendices" >
          Appendices
        </a>
      </li>
      
      <li class="nav-item">
        <a class="nav-link tab-item " id="editors-tab" data-toggle="tab" href="#editors" role="tab" aria-controls="editors" >
          About
        </a>
      </li>
      
    </ul>
  </div>
</nav>
<div class="tab-content" id="masterTabContent">
  
  <div class="tab-pane fade active show" id="index" role="tabpanel" aria-labelledby="index-tab" aria-expanded="true">
    
  <section class="container ms-article-top">
  <div class="ms-row ms-row-single">
    <div class="ms-col-content">
      <h2>Executive summary</h2>
    </div>
  </div>
  <div class="ms-row">
    <div class="ms-col-content">
      <p>Dominant social media platforms are increasingly using automation and AI to find and remove problematic content. While this helps stop some of the worst content from spreading on the Internet, algorithmic content moderation can delete content that should not be deleted (<em>overblocking</em>) or discriminate against minorities. Crucially, there is very little transparency from platforms about how algorithmic content moderation works, how accurate its technologies are believed to be, and how much content they remove, especially without human review. As the use of technologies is likely to only increase, regulators should take the initiative on transparency by requiring platforms to make disclosures.</p>
<p>In order to increase transparency from social media platforms, this policy brief recommends: </p>
<p><strong>Adopting binding and specific disclosure rules</strong> on multiple levels for social media platforms in the use of algorithmic content moderation systems, allowing external oversight and multi-level accountability;</p>
<p><strong>Requiring the implementation of robust and accessible content moderation</strong> <strong>appeal systems</strong> by social media platforms that allow users to appeal against any platforms’ decision on their content, to demand human review if they were subject to an automated individual decision-making, and to quickly reinstate any legitimate content that was wrongly removed in content moderation; and</p>
<p><strong>Establishing a regulatory regime</strong> that involves a multi-stakeholder approach in the rulemaking process, ensuring a more effective and efficient implementation of disclosure rules.</p>

        <h3>Keywords</h3>
        <p>transparency, accountability, platform governance, algorithmic content moderation, multi-level disclosure rules, redress mechanism</p>
        <p class="ms-buttons-article">
            <a class="ms-button ms-button-toc"><i></i> Table of contents</a>
            <a href="#read-full-article" class="ms-button ms-button-read-on"> Read the full brief <i></i></a>
        </p>
    </div>
    <div class="ms-col-marginal">
      <div class="btn-toolbar" role="toolbar" aria-label="Download and share buttons">
        <div class="btn-group mr-auto" role="group" aria-label="Download">
          <a href="https://doi.org/10.5281/zenodo.4292404" class="ms-button ms-button-download"><i></i> PDF</a>
        </div>
        <div class="btn-group" role="group" aria-label="Share">
  <span class="input-group-addon" id="btnGroupAddon">Share</span>
  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//graphite.page/policy-brief-blackbox/" target="_blank"
     class="btn btn-secondary"><i class="mdi mdi-facebook"></i> </a>
  <a href="https://twitter.com/intent/tweet?text=https%3A//graphite.page/policy-brief-blackbox/" target="_blank"
    class="btn btn-secondary"><i class="mdi mdi-twitter"></i> </a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A//graphite.page/policy-brief-blackbox/" target="_blank"
     class="btn btn-secondary"><i class="mdi mdi-linkedin"></i> </a>
</div>
      </div>
      <aside>
        <p><strong>Published on:</strong> 3 Dec 2020</p>
          
            <p><strong>DOI: </strong> 10.5281/zenodo.4292404</p>
          
      </aside>
      <aside>
       
        <p>A policy paper by fellows of the HIIG research sprint on AI and content moderation</p>
        
      </aside>
      <aside>
        <h3>Recommended citation</h3>
        <p>Iramina, A., Spencer-Smith, C., Yan, W. (2020). Disclosure Rules for Algorithmic Content Moderation [Policy Brief]. Research Sprint on AI and Content Moderation. Retrieved from https://graphite.page/policy-brief-blackbox</p>
      </aside>
      
      <aside>
        <h3>Feedback or questions?</h3>
        <p><i class="mdi mdi-email-outline"></i>&nbsp; Write an email to <a href="mailto:vincent.hofmann@hiig.de">vincent.hofmann@hiig.de</a></p>
      </aside>
      
      <a id="read-full-article"></a>
    </div>
  </div>
</section>


<article>
  <div class="container">
     <div class="ms-row ms-row-two ms-text" id="partial_1">
  <div class="ms-col-content">
    
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
</div>


  

    <div class="ms-chapter" style="background-image: url(assets/images/kelly-sikkema-5R5Trsu1aIM-unsplash.jpg)">
      <div class="container-fluid" >
        <div class="container">
          <div class="ms-row ms-row-full" id="partial_2">
            <div class="ms-col-content">
              <h2 id="heading-2">What is the problem?</h2>
<p>The black box phenomenon</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  

<div class="container">
<div class="ms-row ms-row-two ms-text" id="partial_3">
  <div class="ms-col-content">
    <p>Internet platforms, such as Facebook, Twitter, YouTube, WeChat, and TikTok, enable exciting opportunities for expression, but can also be sites of online harms, such as the posting of child abuse imagery and terrorist propaganda, the spread of hate speech and disinformation, and the facilitation of bullying and abusive activity. To counter such online harms, platforms identify problematic content or behaviour and respond by deleting or restricting it, in a process known as content moderation. Alongside human reviewers called "content moderators", platforms use automation and AI to identify and respond to problematic content and behaviour. The benefit of <span class="ms-inline ms-inline-glossary">algorithmic content moderation (ACM)<i></i></span> is that it is a fast and globally scalable way to prevent offensive content being uploaded and travelling across the globe within seconds. It can also spare human content moderators some of the tedium of a very repetitive job, as well as the trauma of viewing the most distressing content, such as child abuse imagery.</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-glossary"><strong><p>ACM:</strong> Platforms use human reviewers, known as content moderators, to screen posts and accounts for abuse. Because of the large amount of activity that happens on platforms everyday, content moderation is too large a task for human content moderators alone. Platforms therefore use technical automation to identify and sanction violating posts and accounts. ACM refers to "systems that classify user-generated content based on either matching or prediction, leading to a decision and governance outcome (e.g. removal, geoblocking, account takedown)".<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Gorwa, R., Binns, R., &amp; Katzenbach, C. (2020). Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data &amp; Society, 7(1).</p>" data-html="true"><i></i></a> This could be as simple as a bot that deletes posts with a certain keyword in them. However, large platforms routinely and increasingly use complex, advanced technologies, such as machine learning (ML), to undertake tasks in content moderation.</p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_4">
  <div class="ms-col-content">
    <p>ACM, however, comes with its own risks. It does not understand context and can either block too much content or too little. "Overblocking" restricts free expression and creativity on the Internet, and can unfairly punish marginalised groups (for example, by misinterpreting certain dialects or vernacular as hate speech).<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Gorwa, R., Binns, R., &amp; Katzenbach, C. (2020). Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data &amp; Society, 7(1).</p>" data-html="true"><i></i></a> Alternatively, ACM can miss abuses that would otherwise be picked up by human reviewers. Although content moderation algorithms play a central role in shaping public discourse and have a concrete impact in the “real” world, platforms release very limited information about them to the public and governments. </p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-keystatement"><p>We should not expect more transparency to be a “magic wand" in itself, but it is an important first step towards appropriate approaches for future regulation and accountability mechanisms.</p></aside>
  </div>
</div>

<figure class="ms-row ms-row-two ms-plugin ms-plugin-figure" id="partial_5">
  <div class="ms-col-content">
    <a href="#5Modal" type="button" data-toggle="modal" data-target="#5Modal">

<img class="img-fluid" src="assets/images/figures/01-policy-paper-blackbox.png" alt="An overview visualising the problem.">

</a>


  </div>
  <div class="ms-col-marginal">
    
<aside class="ms-aside-caption">
  <p><strong>What's the problem?</strong>
      
        <br>The black box phenomenon and its impacts on the user
      
  </p>
</aside>





<aside class="ms-aside-licence">
  <p>CC BY SA 3.0</p>
</aside>


<div class="btn-toolbar" role="toolbar" aria-label="Download and share buttons">
  <div class="btn-group mr-auto" role="group" aria-label="Download">
	<button type="button" class="btn btn-secondary" data-toggle="modal" data-target="#5Modal">
      <i class="mdi mdi-fullscreen"></i>
	</button>
    <a class="btn btn-secondary" href="assets/images/figures/01-policy-paper-blackbox.png"><i class="mdi mdi-download"></i></a>
  </div>
  <div class="btn-group" role="group" aria-label="Share">
    <span class="input-group-addon" id="btnGroupAddon">Share</span>
    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/01-policy-paper-blackbox.png"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-facebook"></i>
    </a>
    <a href="https://twitter.com/intent/tweet?text=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/01-policy-paper-blackbox.png"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-twitter"></i>
    </a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/01-policy-paper-blackbox.png&summary=The+black+box+phenomenon+and+its+impacts+on+the+user&title=What%27s+the+problem%3F"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-linkedin"></i>
    </a>
  </div>
</div>

<div class="modal fade" id="5Modal" tabindex="-1" role="dialog" aria-labelledby="5ModalLabel" aria-hidden="true">
  <div class="modal-dialog" role="document">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">What's the problem?</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <img class="img-fluid" src="assets/images/figures/01-policy-paper-blackbox.png" alt="An overview visualising the problem.">
        
      </div>
    </div>
  </div>
</div>
  </div>
</figure>

<div class="ms-row ms-row-two ms-text" id="partial_6">
  <div class="ms-col-content">
    <p>A lack of transparency prevents public oversight and obstructs regulators from formulating appropriate responses that would protect individual rights and public interests. Furthermore, a lack of transparency produces elevated expectations about the capabilities of algorithms as a "magic wand" for content moderation, while misleading regulators about what kinds of measures can be technically implemented, and which not. We should not expect more transparency to be a “magic wand” in itself, but it is an important first step towards appropriate approaches for future regulation and accountability mechanisms. Major platforms are planning to use more automation and AI in the future, so this issue will become ever more important.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
</div>


  

    <div class="ms-chapter" style="background-image: url(assets/images/kelly-sikkema-_JBGjZFFYRk-unsplash.jpg)">
      <div class="container-fluid" >
        <div class="container">
          <div class="ms-row ms-row-full" id="partial_7">
            <div class="ms-col-content">
              <h2 id="heading-7">What do platforms tell us?</h2>
<p>Platforms’ disclosure of algorithmic content moderation practices</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  

<div class="container">
<div class="ms-row ms-row-two ms-text" id="partial_8">
  <div class="ms-col-content">
    <p>Platforms use a range of different technologies in content moderation, from simple bots that delete content with certain phrases to complex ML algorithms that teach themselves which characteristics to look for when scanning content. Platforms are particularly open about the fact that they use matching technology to block content, such as child abuse imagery and terrorist propaganda. However, they are less open about how they automatically identify and remove other kinds of content which are potentially harmful, but are not illegal.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
</div> <!-- close container -->
<section>
  <div class="container">
    <div class="ms-row ms-row-two ms-plugin ms-plugin-infobox collapse" id="partial_9">
      <div class="ms-col-content">
        <p>Matching technologies aim to detect files that are the same as a file that has already been uploaded. </p>
<p><strong>Hashing</strong> is a technology used to produce a fingerprint (known as a hash) of a multimedia file, which is then matched against a collection of hashes in a database. For example, there are databases of hashed images that have already been identified as child sexual abuse imagery. Images uploaded on platforms can be compared to hashes in this database. If an abusive image is re-uploaded on social media, it can be automatically detected, without a human having to review it.</p>
<p><strong>Classification</strong> technologies aim to find new violations in uploaded content by looking for patterns, e.g. in text or images. Based on pattern identification, the algorithms classify the content into predefined categories, such as nudity/not nudity. This can involve a range of different technologies, such as rules-based AI or ML. For example, platforms can look for hate speech by identifying certain keywords. In a further example, algorithms look for patterns in shapes and colours in an image that indicate that it could contain nudity.</p>
      </div>
      <div class="ms-col-marginal">
        
<aside class="ms-aside-caption">
  <p><strong>What are the main technologies employed in ACM?</strong>
    
  </p>
</aside>





      </div>
    </div>
  </div>
</section>
<div class="container">
<div class="ms-row ms-row-two ms-text" id="partial_10">
  <div class="ms-col-content">
    <p>Platforms publish limited information about the use of ACM on an ad hoc basis, primarily through quarterly or biannual transparency reports about content moderation as a whole. The quality of information contained in these reports varies greatly, especially when it comes to the use of automation and AI. For example, the <a href="https://transparencyreport.google.com/youtube-policy?hl=en">YouTube transparency report</a> publishes the number of videos that are automatically taken down without human review, while the <a href="https://transparency.twitter.com/">Twitter transparency report</a> only contains overall numbers for deleted posts without differentiating between automation and human reviews (see Appendix 3 for more about the information contained in transparency reports). Platforms also publish ad hoc information on their corporate blogs and press releases, but this information is often superficial and difficult to find. Another means is through audit reports by external experts that have been commissioned by the platform, such as the <a href="https://law.yale.edu/sites/default/files/area/center/justice/document/dtag_report_5.22.2019.pdf">Facebook Data Transparency Advisory Group</a>. However, these reports do not give much detailed information and are intended to give recommendations to the platforms, and not to regulators. This way of sharing information does not fit the needs of the general public or regulators.</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-keystatement"><p>Platform transparency reports</p></aside>
<aside class="ms-aside-youtube"><p><a href="https://transparencyreport.google.com/youtube-policy?hl=en">YouTube transparency report</a></p></aside>
<aside class="ms-aside-twitter"><p><a href="https://transparency.twitter.com/">Twitter transparency report</a></p></aside>
<aside class="ms-aside-facebook"><p><a href="https://law.yale.edu/sites/default/files/area/center/justice/document/dtag_report_5.22.2019.pdf">Facebook Data Transparency Advisory Group</a></p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_11">
  <div class="ms-col-content">
    <p>In 2020, TikTok opened a <a href="https://www.tiktok.com/transparency">Transparency Center</a> at their headquarters in Los Angeles, USA. The opening has been delayed because of the COVID-19 crisis, so they are offering virtual tours to journalists and experts, including a look at "[their] safety classifiers and deep learning models that work to proactively identify harmful content and [their] decision engine that ranks potentially violating content to help moderation teams review the most urgent things first".<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Beckman, M. (2020). An update on our virtual Transparency &amp; Accountability Center experience. Retrieved from https://newsroom.tiktok.com/en-us/an-update-on-our-virtual-transparency-and-accountability-center-experience </p>" data-html="true"><i></i></a> This is a step in the right direction. It seems, however, that it will not be open to the public, and TikTok will decide which information it will share, not regulators. It also seems to be focused on the US market, so it might not give insight into content moderation in other parts of the world.</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-keystatement"><p>A significant barrier to transparency is that the information contained in transparency reports (or indeed other information channels) is not standardised across platforms.</p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_12">
  <div class="ms-col-content">
    <p>A significant barrier to transparency is that the information contained in transparency reports (or indeed other information channels) is not standardised across platforms. Platforms have not yet provided direct information about how accurate algorithms are, and very seldom publish how much content is deleted automatically without human review. It is therefore difficult for regulators and the public to understand how much content is being erroneously deleted or being deleted without human intervention. Often, facts are hidden behind obscure metrics, such as <a href="https://transparency.facebook.com/community-standards-enforcement">Facebook’s "proactive rate"</a>, which describes the percentage of content actioned that was discovered before it was reported by a user. This could mean the percentage of content actioned that was identified by algorithms, but it could also include other platform initiatives to find content, so it is not clear. </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>

<figure class="ms-row ms-row-two ms-plugin ms-plugin-figure" id="partial_13">
  <div class="ms-col-content">
    <a href="#13Modal" type="button" data-toggle="modal" data-target="#13Modal">

<img class="img-fluid" src="assets/images/figures/02-policy-paper-blackbox.png" alt="An overview visualising the platform disclosures and their transparency shortfalls.">

</a>


  </div>
  <div class="ms-col-marginal">
    
<aside class="ms-aside-caption">
  <p><strong>Platform disclosures and transparency</strong>
      
        <br>Channels of disclosures and how they fall short of transparency
      
  </p>
</aside>





<aside class="ms-aside-licence">
  <p>CC BY SA 3.0</p>
</aside>


<div class="btn-toolbar" role="toolbar" aria-label="Download and share buttons">
  <div class="btn-group mr-auto" role="group" aria-label="Download">
	<button type="button" class="btn btn-secondary" data-toggle="modal" data-target="#13Modal">
      <i class="mdi mdi-fullscreen"></i>
	</button>
    <a class="btn btn-secondary" href="assets/images/figures/02-policy-paper-blackbox.png"><i class="mdi mdi-download"></i></a>
  </div>
  <div class="btn-group" role="group" aria-label="Share">
    <span class="input-group-addon" id="btnGroupAddon">Share</span>
    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/02-policy-paper-blackbox.png"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-facebook"></i>
    </a>
    <a href="https://twitter.com/intent/tweet?text=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/02-policy-paper-blackbox.png"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-twitter"></i>
    </a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/02-policy-paper-blackbox.png&summary=Channels+of+disclosures+and+how+they+fall+short+of+transparency&title=Platform+disclosures+and+transparency"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-linkedin"></i>
    </a>
  </div>
</div>

<div class="modal fade" id="13Modal" tabindex="-1" role="dialog" aria-labelledby="13ModalLabel" aria-hidden="true">
  <div class="modal-dialog" role="document">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Platform disclosures and transparency</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <img class="img-fluid" src="assets/images/figures/02-policy-paper-blackbox.png" alt="An overview visualising the platform disclosures and their transparency shortfalls.">
        
      </div>
    </div>
  </div>
</div>
  </div>
</figure>

<div class="ms-row ms-row-two ms-text" id="partial_14">
  <div class="ms-col-content">
    <p>Regulators should be aware that platforms use transparency reports to draw attention towards certain topics and away from others. For example, by disclosing numbers of government information and takedown requests, platforms draw attention to the problem of government surveillance and censorship. This issue is deserving of attention, but at the same time, it also draws attention away from the role of platform companies in online speech, including the influence of proprietary content moderation technologies.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Flyverbom, M. (2016). Transparency: Mediation and the Management of Visibilities. International Journal Of Communication, 10(13).</p>" data-html="true"><i></i></a></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
</div>


  

    <div class="ms-chapter" style="background-image: url(assets/images/william-warby-WahfNoqbYnM-unsplash.jpg)">
      <div class="container-fluid" >
        <div class="container">
          <div class="ms-row ms-row-full" id="partial_15">
            <div class="ms-col-content">
              <h2 id="heading-15">Legislative landscape</h2>
<p>Shortfalls of current legislation in ensuring meaningful transparency about the use of algorithmic content moderation systems</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  

<div class="container">
<div class="ms-row ms-row-two ms-text" id="partial_16">
  <div class="ms-col-content">
    <p>Most Internet and platform regulations currently in force do not include provisions that guarantee effective transparency and accountability from digital platforms in the use of their content moderation systems (human or automated). This is particularly true of laws  that provide for limitations on the liability of digital platforms for third-party content. Much of this legislation was introduced around 20 years ago (e.g. <a href="https://uscode.house.gov/view.xhtml?req=(title:47%20section:230%20edition:prelim)">US CDA 230</a>,  and <a href="https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=celex%3A32000L0031">EU E-Commerce Directive</a>), when the use of automated content moderation was still in its infancy. Nonetheless, even more recent laws (e.g. the <a href="https://www2.camara.leg.br/legin/fed/lei/2014/lei-12965-23-abril-2014-778630-publicacaooriginal-143980-pl.html">Brazilian Civil Rights Framework for the Internet</a>) do not directly address the growing importance of new technologies employed by platforms for identifying and filtering illegal content, such as AI.</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-keystatement"><p>Existing legislation on platform regulation, as a rule, still does not take into account the risks involved in the use of AI systems for public discourse, freedom of speech, and other fundamental rights.</p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_17">
  <div class="ms-col-content">
    <p>A great part of the national AI strategies that have been recently adopted or are currently under discussion in different countries highlights the importance of ethics of AI,<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020). Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI.</p>" data-html="true"><i></i></a> including the importance of adopting legal frameworks that ensure that AI applications can be transparent, predictable, and verifiable. However, the existing legislation on platform regulation, as a rule, still does not take into account the risks involved in the use of AI systems for public discourse, freedom of speech, and other fundamental rights. It also treats content moderation by human and automated systems as a single entity, without differentiating them. As a consequence, although some of them provide for general transparency requirements, such as the obligation to publish transparency reports, they fall short in not providing for explicitly mandatory transparency mechanisms in the use of ACM systems, as we can see from some recent national laws and bills that introduce regulations on the topic (see Appendix 1).</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_18">
  <div class="ms-col-content">
    <p>In this context, the <a href="https://gdpr-info.eu/">EU General Data Protection Regulation (GDPR)</a> has already signaled an important change in the thinking around the regulation of automated systems. The GDPR requires the "data controller" to inform “data subjects” when they have been subject to fully automated individual decision-making, which is possible only in the specific cases provided for in article 22. It also introduces mechanisms that allow them to request human review or challenge the decision. Alongside the right to know how platforms handle their personal data, users should also have the right to know how platforms handle their content, especially if they remove it. Now, with the <a href="https://ec.europa.eu/digital-single-market/en/digital-services-act-package">Digital Services Act (DSA),</a> currently under discussion in the European Commission, European regulators seem to be looking for solutions to ensure more transparency and accountability in the use of ACM systems by platforms and set the standards for other countries, as they did with the GDPR. </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
</div>


  

    <div class="ms-chapter" style="background-image: url(assets/images/miguel-a-amutio-9wt0Bytglaw-unsplash.jpg)">
      <div class="container-fluid" >
        <div class="container">
          <div class="ms-row ms-row-full" id="partial_19">
            <div class="ms-col-content">
              <h2 id="heading-19">Policy recommendations</h2>
<p>How to improve the transparency and accountability of algorithmic content moderation systems</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  

<div class="container">
<div class="ms-row ms-row-two ms-text" id="partial_20">
  <div class="ms-col-content">
    <p>It is important to make clear that regulating ACM does not mean legally requiring the use of automated systems. The idea here is that regulators should be aware that algorithmic tools, although necessary, have substantive flaws and often make mistakes, and that platforms, despite their transparency efforts, are not clear about that. Therefore, it is necessary to implement rules that guarantee more transparency in platforms’ decision-making process in content moderation, so it can be subjected to public argumentation and contestation.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Douek, E. (2020). Governing Online Speech: From 'Posts-As-Trumps' to Proportionality and Probability. Columbia Law Review, 121(1).</p>" data-html="true"><i></i></a> Moreover, governments should have access to this information, so they can make informed decisions in their regulatory responses to the problems that might emerge in the process. For that, specific and binding disclosure rules should be the first step for more transparency.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_21">
  <div class="ms-col-content">
    <p>This does not mean, however, that all data to be disclosed by platforms should necessarily be provided for in law, especially in light of the technical aspects involved in the use of algorithmic systems and their rapid technological development. While lawmakers should work on provisions that clearly define platforms’ transparency obligations and explicitly enforce data disclosure on the use of these systems, with legal safeguards against the undermining of privacy, freedom of speech, and other fundamental rights, they should also leave room for further regulation by the relevant public authority with appropriate technical and policy expertise. </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_22">
  <div class="ms-col-content">
    <p>This public authority could be an existing authority, preferably an independent public authority with key transparency and due processes obligations,<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>ARTICLE 19. Article 19’s recommendation for the EU Digital Services Act.</p>" data-html="true"><i></i></a> that would absorb these new competences related to the implementation of binding disclosure rules. However, it should work in a multi-stakeholder arrangement, with a committee, panel, or group composed of interested parties, which would be responsible for providing assistance and advice in the formulation of common technical standards (e.g. metrics) and statutory instruments that clearly establish how and what kind of data should be disclosed by platforms. This would guarantee more flexibility and technical expertise in the implementation of disclosure rules. That is why a legal requirement for the implementation of content moderation appeal systems by platforms and the establishment of a regulatory regime  that involves a multi-stakeholder approach in the rulemaking process are also recommended in this policy brief. The increasing deployment of ACM should be accompanied by governmental oversight, accountability, and appeal mechanisms.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Mozilla Foundation (2020). Digital Services Act package: open public consultation. </p>" data-html="true"><i></i></a> </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>

<figure class="ms-row ms-row-two ms-plugin ms-plugin-figure" id="partial_23">
  <div class="ms-col-content">
    <a href="#23Modal" type="button" data-toggle="modal" data-target="#23Modal">

<img class="img-fluid" src="assets/images/figures/03-policy-paper-blackbox.png" alt="An overview summarising the proposal for increased transparency and accountability.">

</a>


  </div>
  <div class="ms-col-marginal">
    
<aside class="ms-aside-caption">
  <p><strong>Proposals to increase transparency and accountability</strong>
      
        <br>Disclosure obligations, content moderation appeal mechanisms and regulatory regimes
      
  </p>
</aside>





<aside class="ms-aside-licence">
  <p>CC BY SA 3.0</p>
</aside>


<div class="btn-toolbar" role="toolbar" aria-label="Download and share buttons">
  <div class="btn-group mr-auto" role="group" aria-label="Download">
	<button type="button" class="btn btn-secondary" data-toggle="modal" data-target="#23Modal">
      <i class="mdi mdi-fullscreen"></i>
	</button>
    <a class="btn btn-secondary" href="assets/images/figures/03-policy-paper-blackbox.png"><i class="mdi mdi-download"></i></a>
  </div>
  <div class="btn-group" role="group" aria-label="Share">
    <span class="input-group-addon" id="btnGroupAddon">Share</span>
    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/03-policy-paper-blackbox.png"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-facebook"></i>
    </a>
    <a href="https://twitter.com/intent/tweet?text=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/03-policy-paper-blackbox.png"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-twitter"></i>
    </a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//graphite.page/policy-brief-blackbox/assets/images/figures/03-policy-paper-blackbox.png&summary=Disclosure+obligations%2C+content+moderation+appeal+mechanisms+and+regulatory+regimes&title=Proposals+to+increase+transparency+and+accountability"
       target="_blank" class="btn btn-secondary">
      <i class="mdi mdi-linkedin"></i>
    </a>
  </div>
</div>

<div class="modal fade" id="23Modal" tabindex="-1" role="dialog" aria-labelledby="23ModalLabel" aria-hidden="true">
  <div class="modal-dialog" role="document">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Proposals to increase transparency and accountability</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <img class="img-fluid" src="assets/images/figures/03-policy-paper-blackbox.png" alt="An overview summarising the proposal for increased transparency and accountability.">
        
      </div>
    </div>
  </div>
</div>
  </div>
</figure>

<div class="ms-row ms-row-two ms-text" id="partial_24">
  <div class="ms-col-content">
    <h3 id='heading-24' >Adoption of multi-level disclosure rules</h3>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_25">
  <div class="ms-col-content">
    <p>The adoption of multi-level specific and binding disclosure rules is an important mechanism for increasing transparency, without excessively intervening in platforms’ business models.  Mandatory transparency reports with only general provisions on what should be disclosed have proven insufficient. Without specific and binding disclosure obligations, platforms decide what data and information to disclose and how they will disclose them, which, in many ways, obstruct independent studies and external oversight. The option to provide for disclosure obligations on multiple levels allows governments to share oversight responsibilities with different actors (e.g. civil society, academia, and users), opting for a multi-level accountability regime.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Leerssen, P. (2020). The Soap Box as a Black Box: Regulating Transparency in Social Media Recommender Systems.</p>" data-html="true"><i></i></a> Independent researchers should be able to have access to data that allow them to audit algorithms and undertake impact assessments of these algorithms, e.g. for public discourse and freedom of speech. <span class="ms-inline ms-inline-sidenote">Suggestions<i></i></span> for multi-level disclosure:</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-sidenote"><p>In its suggestion for tiered disclosure, this paper draws significantly upon<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Leerssen, P. (2020). The Soap Box as a Black Box: Regulating Transparency in Social Media Recommender Systems.</p>" data-html="true">Paddy Leerssen’s (2020)<i></i></a> suggestion for tiered disclosures in social media recommender systems. Although closely interconnected, regulators should take care to differentiate between recommender systems and content moderation systems. For example, considerations of media diversity are relevant to recommender systems (e.g. are users being exposed to a sufficiently diverse array of news sources?), whereas this applies less to content moderation. Furthermore, publicly available tools providing insight into which content is recommended are already in existence (see algotransparency.org), whereas databases of certain categories of removed content should not be made publicly available for ethical reasons (such as child abuse imagery and image based abuse, also known as "revenge porn").</p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_26">
  <div class="ms-col-content">
    <p><strong>User-facing disclosure:</strong> In addition to notifying the user of a takedown and offering the option to appeal, platforms should be required to notify users if the takedown is a result of an automated decision without human review.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_27">
  <div class="ms-col-content">
    <p><strong>Civil society and research disclosure:</strong> Platforms should be required to allow researchers access to algorithmic tools for the purposes of algorithmic auditing<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Schmon, C. (2020). EFF Responds to EU Commission on the Digital Services Act: Put Users Back in Control.</p>" data-html="true"><i></i></a> and archived databases of deleted content, along with records of their efforts to tackle that content.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Bowers, J. &amp; Zittrain, J. (2020). Answering impossible questions: content governance in an age of disinformation. The Harvard Kennedy School (HKS) Misinformation Review.</p>" data-html="true"><i></i></a> This should be available for most categories of account, such as copyright infringement, hate speech, and disinformation. For certain categories, such as child sexual abuse imagery and image-based abuse ("revenge porn"), there are ethical arguments against providing access. Such considerations could be negotiated in a co-regulatory structure.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_28">
  <div class="ms-col-content">
    <p><strong>General disclosure:</strong> Transparency standards should be imposed on large commercial platforms requiring information to be disclosed in a standardised form, while also allowing for differences between platforms (e.g. differences in community guidelines, or what counts as a violation). This includes provisions that oblige platforms to incorporate in their transparency reports particular information, such as: </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_29">
  <div class="ms-col-content">
    <ul>
<li>Explanation of how automated detection is used for each category of rule violated,<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Santa Clara Principles (2018). Santa Clara Principles. </p><a class='mdi mdi-earth' href='https://santaclaraprinciples.org' target='_blank'></a>" data-html="true"><i></i></a><a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>European Regulators Group for Audiovisual Media Services (2020). ERGA Position Paper on the Digital Services Act. ERGA 2020 Subgroup 1 - Enforcement.</p>" data-html="true"><i></i></a> what types of tools they are using and for what purposes, and what automated decision making approach each tool uses (e.g. natural language processing, hashing, etc.).</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_30">
  <div class="ms-col-content">
    <ul>
<li>Numbers of posts and accounts flagged by algorithms for each category, broken down by country.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>European Regulators Group for Audiovisual Media Services (2020). ERGA Position Paper on the Digital Services Act. ERGA 2020 Subgroup 1 - Enforcement.</p>" data-html="true"><i></i></a></li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_31">
  <div class="ms-col-content">
    <ul>
<li>Numbers of posts and accounts deleted automatically for each category, broken down by country.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>European Regulators Group for Audiovisual Media Services (2020). ERGA Position Paper on the Digital Services Act. ERGA 2020 Subgroup 1 - Enforcement.</p>" data-html="true"><i></i></a></li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_32">
  <div class="ms-col-content">
    <ul>
<li>Estimated accuracy rates for flagging and how accuracy is defined.</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_33">
  <div class="ms-col-content">
    <ul>
<li>As content moderation is geographically patchy across the globe, platforms should also be required to disclose roll-outs of new AI systems (e.g. the AI system being rolled out in the jurisdiction and for which kinds of violations, e.g. new roll-out of AI against hate speech in a particular country).</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_34">
  <div class="ms-col-content">
    <h3 id='heading-34' >Implementing content moderation appeal systems</h3>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_35">
  <div class="ms-col-content">
    <p>The implementation of robust and accessible content moderation appeal systems is considered an essential mechanism to guarantee more accountability from platforms. It is in line with users’ right to information, particularly to be notified when they are subject to automated decisions without human review, since they should also have the right, for example, to appeal against any decision they consider wrong.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>Santa Clara Principles (2018). Santa Clara Principles. </p><a class='mdi mdi-earth' href='https://santaclaraprinciples.org' target='_blank'></a>" data-html="true"><i></i></a> ACM without human review entails the risk of content being wrongly removed, and as we have seen during COVID-19 crisis, its use is on the increase. Appeal mechanisms have become even more necessary to guarantee the rapid reinstatement of any legitimate content or account that was wrongly removed, mitigating potential harmful effects for public discourse, freedom of speech, and other users rights.  </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_36">
  <div class="ms-col-content">
    <h3 id='heading-36' >Establishing a multi-stakeholder-based regulatory regime</h3>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_37">
  <div class="ms-col-content">
    <p>The establishment of a regulatory regime that involves a multi-stakeholder approach in the rulemaking process is also a key mechanism to guarantee more effectiveness and efficiency in the implementation, regulation, monitoring, and enforcement of disclosure rules. The relevant public authority should include in its decision-making process a multi-stakeholder group, with representatives of interested parties, such as platforms, independent researchers, users, and civil society organizations. This multi-stakeholder group should provide assistance and advice to the public authority in drafting common disclosure standards and rules. It could also assist the public authority in monitoring whether platforms are complying with their disclosure obligations in a timely, accurate, and complete manner. It could, for example, advise on cases involving non-compliance complaints against platforms and the application of sanctions in case of non-compliance. There is not one single model for the adoption of a multi-stakeholder approach. This multi-stakeholder group could be part of the public authority structure or an external advisory group, depending on the existing infrastructure or the preferences of the country or region in question. Governments have already adopted this kind of structure for other regulated sectors, so they can look at them as a reference.</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-keystatement"><p>"Multistakeholder approach is a set of tools or practices that all share one basis: Individuals and organizations from different realms participating alongside each other to
share ideas or develop consensus policy"</p></aside>
<aside class="ms-aside-link"><p><a href="https://www.internetsociety.org/resources/doc/2016/internet-governance-why-the-multistakeholder-approach-works">Internet Governance – Why the Multistakeholder Approach Works</a></p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_38">
  <div class="ms-col-content">
    <p>There are some other suggestions that also propose a multi-stakeholder approach, particularly from civil society groups. One example is the creation of an independent, accountable, and transparent multi-stakeholder body, which would be responsible for elaborating and implementing the technical and practical remedies necessary to guarantee more transparency and accountability from platforms, as set in the law.<a tabindex="0" data-trigger="focus hover" class="ms-inline ms-inline-reference" data-toggle="popover" data-placement="top" title="Cited source" data-content="<p>ARTICLE 19. Article 19’s recommendation for the EU Digital Services Act.</p>" data-html="true"><i></i></a> In this case, some of the competences of the relevant public authority would be transferred to the independent multi-stakeholder body, which would be monitored by this public authority. This would also be a viable solution and, depending on the public structure and resources available in each country, using existing infrastructure and involving directly existing stakeholders could be an option to reduce some of the costs involved in the implementation of these rules by states. </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_39">
  <div class="ms-col-content">
    <p>In order to achieve the expected results with the adoption of these transparency and accountability mechanisms, one of the biggest challenges for governments will be to gather the knowledge and technical expertise necessary to implement them. That is why it is so important to have a multi-stakeholder approach already in the regulatory phase. The option of sharing responsibilities with other stakeholders through a multi-level accountability regime, in our view, is one of the best solutions to tackle platforms’ lack of transparency in the use of ACM systems. </p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_40">
  <div class="ms-col-content">
    
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
  </div>
</article>
  </div>
  
  <div class="tab-pane fade" id="authors" role="tabpanel" aria-labelledby="authors-tab" >
    

<section>
  <div class="container">
     <div class="ms-row ms-row-two ms-text" id="partial_41">
  <div class="ms-col-content">
    <h2 id='heading-41' >Authors</h2>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_42">
  <div class="ms-col-content">
    <p>The three authors of this brief were all fellows of the first research sprint and contributed equally to the formulation of the policy brief.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>

<div class="ms-row ms-row-three ms-plugin ms-plugin-author" id="partial_43">
  <div class="ms-col-pre">
    <img class="img-fluid" src="assets/images/authors/Aline.png" alt="">
  </div>
  <div class="ms-col-content">
    <p>
  <strong>Aline Iramina</strong>
</p>
<p class="grey">
  University of Brasilia, Brazil, & University of Glasgow, United Kingdom
</p>

<p>I am a Ph.D. candidate in Law at the University of Brasilia (Brazil) and Ph.D. candidate in Law and researcher at the University of Glasgow (UK). What fascinates me the most about the field of AI and content moderation is its growing influence on the way we communicate, consume information, products, and services, and relate to each other on the Internet. Besides that, as a copyright specialist, content moderation for the purpose of copyright enforcement has been a matter of discussion at least in the last two decades. More recently, with the development of new algorithms, it has gained even more relevance for us who work in this field. Currently, everybody talks about the necessity of guaranteeing that people, companies, and even AI applications behave ethically on the Internet, even though most of us do not even know what that necessarily means. This is why it is so important to address this issue and, as researchers, we have the responsibility to bring more clarity on that.</p>

  </div>
  <div class="ms-col-marginal">
    


<aside class="ms-aside-twitter">
  <p>
    <a href="https://twitter.com/IraAline" target="_blank">
       <span class="grey">@&thinsp;</span>IraAline
    </a>
  </p>
</aside>


<aside class="ms-aside-linkedin">
  <p><a href="https://www.linkedin.com/in/aline-iramina-4758244b" target="_blank">
       <span class="grey">in&thinsp;/&thinsp;</span>aline-iramina-4758244b
    </a></p>
</aside>



  </div>
</div>


<div class="ms-row ms-row-three ms-plugin ms-plugin-author" id="partial_44">
  <div class="ms-col-pre">
    <img class="img-fluid" src="assets/images/authors/Charlotte Spencer-Smith.jpg" alt="">
  </div>
  <div class="ms-col-content">
    <p>
  <strong>Charlotte Spencer-Smith</strong>
</p>
<p class="grey">
  University of Salzburg, Austria
</p>

<p>I am a doctoral candidate at the doctoral candidate at the Center for Information and Communication Technologies & Society (ICT&S) at the Department of Communication Studies at the University of Salzburg in Austria. I’m interested in the architecture, algorithms and affordances of online platforms, particularly social effects of recommendation systems and content moderation. The legal scholar Daphne Keller says: “no communications medium in human history has ever worked in this way”. Automation technologies and artificial intelligence are very likely to increase their influence on online life. Young people coming of age in 2020 don’t really have the option to opt-out, so we should advocate for a humane approach to technological change. As researchers, our power is in giving voice to users, be that through empirical research or policy work.</p>

  </div>
  <div class="ms-col-marginal">
    

<aside class="ms-aside-link">
  <p>
    <a href=https://kowi.uni-salzburg.at/ma/spencer-smith-charlotte target="_blank">
       Visit website
    </a>
  </p>
</aside>





  </div>
</div>


<div class="ms-row ms-row-three ms-plugin ms-plugin-author" id="partial_45">
  <div class="ms-col-pre">
    <img class="img-fluid" src="assets/images/authors/Wai-Yan.jpg" alt="">
  </div>
  <div class="ms-col-content">
    <p>
  <strong>Wai Yan</strong>
</p>
<p class="grey">
  Koe Koe Tech, Myanmar
</p>

<p>My name is Wai Yan, and I work as a lead AI engineer at Koe Koe Tech. I was recommended by my boss to this research sprint. I find AI and content moderation important because it is shaping society and mass behaviour. As a researcher, I would like people to understand how online social interactions are being guided by algorithms and why it is important that platforms are properly regulated. We need to talk more about ethics because it is the basis on which law is made, and platforms without ethics are bound to cause problems in society.</p>

  </div>
  <div class="ms-col-marginal">
    



<aside class="ms-aside-linkedin">
  <p><a href="https://www.linkedin.com/in/william-zhu-73b08437" target="_blank">
       <span class="grey">in&thinsp;/&thinsp;</span>william-zhu-73b08437
    </a></p>
</aside>



  </div>
</div>

<div class="ms-row ms-row-two ms-text" id="partial_46">
  <div class="ms-col-content">
    
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
  </div>
</section>
  </div>
  
  <div class="tab-pane fade" id="directories" role="tabpanel" aria-labelledby="directories-tab" >
    

<section>
  <div class="container">
     <div class="ms-row ms-row-two ms-text" id="partial_47">
  <div class="ms-col-content">
    <h2 id='heading-47' >Sources</h2>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-directory" id="partial_48">
  <div class="ms-col-content">
    
<div class="ms-entry">
  <p>
    <p>AlgorithmWatch (2020). Our response to the European Commission’s planned Digital Services Act. Retrieved from https://algorithmwatch.org/en/submission-digital-services-act-dsa/#audit</p>
    
    <a class="mdi mdi-earth" href="https://algorithmwatch.org/en/submission-digital-services-act-dsa/#audit" target="_blank"></a>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>ARTICLE 19. Article 19’s recommendation for the EU Digital Services Act. Retrieved from https://www.article19.org/wp-content/uploads/2020/04/ARTICLE-19s-Recommendations-for-the-EU-Digital-Services-Act-FINAL.pdf</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Ausloos, J., Leerssen, P., Thije, P. (2020). Operationalizing Research Access in Platform Governance What to learn from other industries?. Retrieved from https://www.ivir.nl/publicaties/download/GoverningPlatforms_IViR_study_June2020-AlgorithmWatch-2020-06-24.pdf</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Beckman, M. (2020). An update on our virtual Transparency &amp; Accountability Center experience. Retrieved from https://newsroom.tiktok.com/en-us/an-update-on-our-virtual-transparency-and-accountability-center-experience </p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Bowers, J. &amp; Zittrain, J. (2020). Answering impossible questions: content governance in an age of disinformation. The Harvard Kennedy School (HKS) Misinformation Review. Retrieved from https://misinforeview.hks.harvard.edu/article/content-governance-in-an-age-of-disinformation/</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Lei n° 12965 de 23 de abril de 2014 estabelece princípios garantias direitos e deveres para o uso da Internet no Brasil (BRA)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Criminal Code Amendment (sharing of abhorrent violent material) Act 2019 (AUS)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Communications Decency Act of 1996 47 USC § 230</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of information society services, in particular electronic commerce, in the Internal Market  [2000] OJ L 178/1</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Douek, E. (2020). Governing Online Speech: From 'Posts-As-Trumps' to Proportionality and Probability. Columbia Law Review, 121(1). Retrieved from http://dx.doi.org/10.2139/ssrn.3679607</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC  [2016] OJ L 119/1</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>European Regulators Group for Audiovisual Media Services (2020). ERGA Position Paper on the Digital Services Act. ERGA 2020 Subgroup 1 - Enforcement. Retrieved from https://erga-online.eu/wp-content/uploads/2020/06/ERGA_SG1_DSA_Position-Paper_adopted.pdf</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Hate Speech and Disinformation Prevention and Suppression Proclamation No.1185 /2020 (ETH)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>European Parliament (2020). Report with recommendations to the Commission on a Digital Services Act: adapting commercial and civil law rules for commercial entities operating online. Retrieved from https://www.europarl.europa.eu/doceo/document/A-9-2020-0177_EN.html#title1</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Federal Law on Amendments to Article 10 of Federal Law n. 149-FZ of July 27, 2006 on Information, Informational Technologies and the Protection of Information (RUS)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020). Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI. Retrieved from  http://nrs.harvard.edu/urn-3:HUL.InstRepos:42160420</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Flyverbom, M. (2016). Transparency: Mediation and the Management of Visibilities. International Journal Of Communication, 10(13). Retrieved from  https://ijoc.org/index.php/ijoc/article/view/4490/1531</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Global Network Initiative (2020). Digital Services Act package: open public consultation. Retrieved from https://globalnetworkinitiative.org/wp-content/uploads/2020/09/DSA-Survey-GNI-Submission-1.pdf</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Gorwa, R., Binns, R., &amp; Katzenbach, C. (2020). Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data &amp; Society, 7(1). Retrieved from https://doi.org/10.1177/2053951719897945</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Intervozes, Observacom, Idec and Desarrollo Digital (2019). Contribuições para uma Regulação Democrática das Grandes Plataformas que garanta a Liberdade de Expressão na Internet. Retrieved from https://www.observacom.org/wp-content/uploads/2019/08/Contribuic%cc%a7o%cc%83es-para-uma-regulac%cc%a7a%cc%83o-democra%cc%81tica-das-grandes-plataformas-que-garanta-a-liberdade-de-expressa%cc%83o-na-internet.pdf</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Leerssen, P. (2020). The Soap Box as a Black Box: Regulating Transparency in Social Media Recommender Systems. Retrieved from http://dx.doi.org/10.2139/ssrn.3544009</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Marsden, C. (2011). Internet co-regulation and constitutionalism. In Internet Co-Regulation: European Law, Regulatory Governance and Legitimacy in Cyberspace (pp. 46-70). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511763410.003</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Mozilla Foundation (2020). Digital Services Act package: open public consultation. Retrieved from https://ffp4g1ylyit3jdyti1hqcvtb-wpengine.netdna-ssl.com/netpolicy/files/2020/09/Contribution1ad6cb23-a986-4b8c-9a08-80d11e5b0d47.pdf</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Gesetz zur Verbesserung der Rechtsdurchsetzung in sozialen Netzwerken [Netzwerksdurchsetzungsgesetz] [NetzDG]) (DEU)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Protection from Online Falsehoods and Manipulation Act (No. 18 of 2019) (SGP)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Santa Clara Principles (2018). Santa Clara Principles. Retrieved from https://santaclaraprinciples.org</p>
    
    <a class="mdi mdi-earth" href="https://santaclaraprinciples.org" target="_blank"></a>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>Schmon, C. (2020). EFF Responds to EU Commission on the Digital Services Act: Put Users Back in Control. Retrieved from https://www.eff.org/deeplinks/2020/09/eff-responds-eu-commission-digital-services-act-put-users-back-control</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>İnternet Ortamında Yapılan Yayınların Düzenlenmesi ve Bu Yayınlar Yoluyla İşlenen Suçlarla Mücadele Edilmesi Hakkında Kanun. T.C. Resmî Gazete. 23 Mayıs 2007. 3 Ağustos 2011 tarihinde kaynağından arşivlendi. Erişim tarihi: 29 Temmuz 2020 (TUR)</p>
    
  </p>
</div>

<div class="ms-entry">
  <p>
    <p>UK Home Office and UK Department for Digital, Culture, Media &amp; Sport (2020). Online Harms White Paper - Initial Consultation response. Retrieved from https://www.gov.uk/government/consultations/online-harms-white-paper/public-feedback/online-harms-white-paper-initial-consultation-response#contents</p>
    
  </p>
</div>

  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-title">
</aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_49">
  <div class="ms-col-content">
    
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
  </div>
</section>
  </div>
  
  <div class="tab-pane fade" id="appendices" role="tabpanel" aria-labelledby="appendices-tab" >
    

<section>
  <div class="container">
     <div class="ms-row ms-row-two ms-text" id="partial_50">
  <div class="ms-col-content">
    <h3 id='heading-50' >Appendix 1</h3>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>

<div class="ms-row ms-row-two ms-plugin ms-plugin-table" id="partial_51">
  <div class="ms-col-content">
    <table border="0" class="dataframe table table-hover">
  <thead class="thead-inverse">
    <tr style="text-align: left;">
      <th>Country</th>
      <th>Law/Bill</th>
      <th>Hate Speech moderated</th>
      <th>Fake News moderated</th>
      <th>Other harmful or illegal content moderated</th>
      <th>Mandatory transparency measures (e.g. transparency reports)</th>
      <th>AI and algorithmic transparency measures</th>
    </tr>
  </thead>
  <tbody class="tcol-head">
    <tr>
      <td><p>Australia</p></td>
      <td><p>Criminal Code Amendment (sharing of abhorrent violent material)</p></td>
      <td><p>x</p></td>
      <td></td>
      <td><p>x</p></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Austria</p></td>
      <td><p>Draft on measures to protect users on communication platforms (Communications Platforms Act)</p></td>
      <td><p>x</p></td>
      <td></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Brazil</p></td>
      <td><p>Brazilian Law on Freedom, Responsibility and Transparency on the Internet (Bill No. 2630/2020)</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Ethiopia</p></td>
      <td><p>Hate Speech and Disinformation Prevention and Suppression Proclamation No.1185 /2020</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Germany</p></td>
      <td><p>German Social Networks Enforcement Act (NetzDG)</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
    </tr>
    <tr>
      <td><p>India</p></td>
      <td><p>The Information Technology Intermediaries Guidelines (amendment) rules, 2018</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Russia</p></td>
      <td><p>Federal Law on Amendments to Article 10 of Federal Law on Information, Information Technologies and the Protection of Information</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Singapore</p></td>
      <td><p>Protection from Online Falsehoods and Manipulation Act (No. 18 of 2019)</p></td>
      <td></td>
      <td><p>x</p></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><p>Turkey</p></td>
      <td><p>Law No. 7253 amending “Law No. 5651 on The Regulation of Publications made in the Internet Environment and Combatting Crimes Committed through these Publications.”</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td><p>x</p></td>
      <td></td>
    </tr>
  </tbody>
</table>
  </div>
  <div class="ms-col-marginal">
    
<aside class="ms-aside-caption">
  <p><strong>Laws and Bills with legal provisions on content moderation by social media platforms</strong></p>
</aside>


<aside class="ms-aside-description">
  <p><p><em>Law/Bill</em>: national laws introduced in the last 4 years and bills currently under discussion that introduce regulation on content moderation by social media platforms. </p>
<p>There are some cases where the content addressed by the law is not clear, particularly  in relation to fake news. </p>
<p><em>Transparency measures</em> in the use of AI and algorithms for content moderation, not for other purposes. Some laws or bills include among the information to be provided by platforms, for example, methods or methodology employed in the detection of irregularity”, which could include information on algorithms and AI, but it is not clear about that.  </p></p>
</aside>

<aside class="ms-aside-download">
  <p><a href="assets/tables/appendix-a.csv">Download .csv</a></p>
</aside>
  </div>
</div>

<div class="ms-row ms-row-two ms-text" id="partial_52">
  <div class="ms-col-content">
    <h3 id='heading-52' >Appendix 2</h3>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_53">
  <div class="ms-col-content">
    <p>Information disclosed about ACM in transparency reports per platform (based on most recent reports as of October 2020)</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_54">
  <div class="ms-col-content">
    <p><strong>Facebook &amp; Instagram</strong></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_55">
  <div class="ms-col-content">
    <ul>
<li>“Proactive rate”: percentage of content actioned that was found before users report it. This includes “detection technology”, but it is vague as to whether the “proactive rate” means content removed automatically, or if content removed by other means is also included. </li>
<li>Appeal rates: content deletions appealed by the user. This cannot be taken as a proxy for accuracy, because users do not necessarily appeal when content is wrongfully removed. </li>
<li>Reinstatement rates: removed content that is later restored, either as a result of appeal or because Facebook has independently decided to restore it. This also cannot be taken as a proxy for accuracy, because the number depends, in part, on the appeal rate.</li>
<li>Rates are given by content category, but not by country.
Occasionally, blog posts that accompany the release of a new transparency report will make mention of new developments in technology, such as the roll-out of a new AI in a geographic region.</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_56">
  <div class="ms-col-content">
    <p><strong>Reddit</strong></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_57">
  <div class="ms-col-content">
    <ul>
<li>Content moderation relies mostly on Reddit admins, who set the Content Policy, and community moderators, who set Community Rules specific to a community called a subreddit. Reddit also has a tool called AutoModerator, which can be customized by the moderator to set rules and help them in moderation, e.g. reporting or removing comments containing certain phrases or links.</li>
<li>The Reddit transparency report 2019 publishes overall content removals and appeals as well as government requests for content removal and user information. Regarding ACM, it only mentions that they use PhotoDNA and YouTube CSAI Match to detect child sexual abuse imagery.</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_58">
  <div class="ms-col-content">
    <p><strong>YouTube (Google)</strong></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_59">
  <div class="ms-col-content">
    <ul>
<li>YouTube reports the number of videos removed by automated flagging. However, this only includes videos removed because they violate YouTube’s own rules, and does not include videos that are deemed to be illegal in various countries. Numbers for copyright infringement are not reported.</li>
<li>Appeal and reinstatement rates are reported, but these cannot be seen as a proxy for accuracy rates.</li>
<li>Rates are given by content category and by country, but only for overall takedowns.</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_60">
  <div class="ms-col-content">
    <p><strong>Twitter</strong></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_61">
  <div class="ms-col-content">
    <ul>
<li>Twitter does not publish information about automation, algorithms, or AI in its content moderation transparency reports. Instead, it publishes overall numbers of accounts actioned, content removed, and accounts suspended.</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_62">
  <div class="ms-col-content">
    <p><strong>TikTok</strong></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_63">
  <div class="ms-col-content">
    <ul>
<li>TikTok gives the total global number of videos flagged and removed automatically for violating their guidelines. This number is not broken down by country or category.</li>
<li>TikTok breaks down numbers per category and country, but only for overall takedowns (including means other than ACM). It also excludes the number of automatic removals from its country breakdowns.</li>
</ul>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_64">
  <div class="ms-col-content">
    
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
  </div>
</section>
  </div>
  
  <div class="tab-pane fade" id="editors" role="tabpanel" aria-labelledby="editors-tab" >
    

<section>
  <div class="container">
     <div class="ms-row ms-row-two ms-text" id="partial_65">
  <div class="ms-col-content">
    <h2 id='heading-65' >The Ethics of Digitalisation</h2>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_66">
  <div class="ms-col-content">
    <p><strong>This policy brief forms one of the three outputs of the research sprint on AI and content moderation which was hosted virtually by the HIIG from August to October 2020.</strong><br>
The NoC (Global Network of Internet and Society Research Centers) research project <em>The Ethics of Digitalisation - From Principles to Practices</em> promotes an active exchange and aims to foster a global dialogue on the ethics of digitalisation by involving stakeholders from academia, civil society, policy, and the industry. Research sprints and clinics form the core of the project; they enable interdisciplinary scientific work on application-, and practice-oriented questions and challenges and achieve outputs of high social relevance and impact. The first research sprint on AI and content moderation brought together thirteen fellows from nine different countries across seven different  time zones. Their academic expertise ranged from law and public policy to data science and digital ethics and together they explored key challenges arising from the use of automation and machine learning in content moderation.</p>
  </div>
  <div class="ms-col-marginal">
    <aside class="ms-aside-keystatement"><h3>Imprint</h3>
<p>Alexander von Humboldt Institut für Internet und Gesellschaft<br><span class="light">Französische Str. 9<br>10117 Berlin</span></p>
<p><span class="light">Responsibe according to §&nbsp;55&nbsp;Abs.&nbsp;2&nbsp;RStV: Dr. Karina Preiß</span></p>
<p><span class="light"><a href="https://www.hiig.de/en/imprint">Imprint on hiig.de</a></span></p></aside>
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_67">
  <div class="ms-col-content">
    <h3 id='heading-67' >Team</h3>
<p><strong>The research sprint and publications were organised and reviewed by the project team:</strong><br>
<a href="https://www.hiig.de/en/nadine-birner">Nadine Birner</a>, project coordinator of The Ethics of Digitalisation<br>
<a href="https://www.hiig.de/en/christian-katzenbach">Christian Katzenbach</a>, head of the HIIG research programme: The evolving digital society<br>
<a href="https://www.hiig.de/en/matthias-kettemann">Matthias Kettemann</a>, researcher at Leibniz-Institut für Medienforschung | Hans-Bredow-Institut and associated researcher at HIIG<br>
<a href="https://www.hiig.de/en/alexander-pirang">Alexander Pirang</a>, researcher at AI &amp; Society Lab<br>
<a href="https://www.hiig.de/en/friederike-stock">Friederike Stock</a>, student assistant of The Ethics of Digitalisation</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_68">
  <div class="ms-col-content">
    <p><strong>Design and implementation</strong><br>
<a href="https://www.larissawunderlich.de">Larissa Wunderlich</a></p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_69">
  <div class="ms-col-content">
    <p>The publication was build with the open-source framework <a href="https://www.impactdistillery.com/graphite">graphite</a> developed by <a href="https://www.impactdistillery.com">Marcel Hebing</a> and <a href="https://www.larissawunderlich.de">Larissa Wunderlich</a>.</p>
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
<div class="ms-row ms-row-two ms-text" id="partial_70">
  <div class="ms-col-content">
    
  </div>
  <div class="ms-col-marginal">
    
  </div>
</div>
  </div>
</section>
  </div>
  
</div>
<canvas id="confetti"></canvas>
<!-- END: STUDY FRAME -->


    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.bundle.min.js"></script>
    <script src="/static/js/jquery.actual.min.js"></script>
    <script src="/static/js/horst.js"></script>
    <script src="/static/js/youtube.js"></script>
    <script src="/static/js/confetti.js"></script>
    
    
      <!-- Matomo -->
      <script type="text/javascript">
        var _paq = window._paq = window._paq || [];
        /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
          var u="https://piwik.wunderjewel.de/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '4']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.type='text/javascript'; g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
        })();
      </script>
      <!-- End Matomo Code -->
    
  </body>
</html>
